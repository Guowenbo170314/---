#应用jieba自带的analyse计算TF-IDF值，获取特征词
#以十九届四中全会通报为例，基于TF-IDF进行特征词提取
path='D:\\十九届四中全会.txt'
with open(path,encoding='utf-8') as fp:
    text=fp.read()
#删除空格，换行，返回list    text=fp.read().split() 
a=jieba.analyse.extract_tags(text, topK=20, withWeight=False, allowPOS=())
print(a)
#以十九届四中全会通报为例，提取高频词汇
path='D:\\十九届四中全会.txt'
with open(path,encoding='utf-8') as fp:
    text=fp.read()
words = jieba.cut(text, cut_all = False)
word_freq = {}
for word in words:
    if word in word_freq:
        word_freq[word] += 1
    else:
        word_freq[word] = 1
freq_word = []
for word, freq in word_freq.items():
        freq_word.append((word, freq))
freq_word.sort(key = lambda x: x[1], reverse = True)   #反序排列，根据第二个参数（频数）
for word, freq in freq_word[: 10]:    #10为可设置的最多输出几个高频词汇
        print(word, freq)

#使用scikit-learn工具计算文本TF-IDF值，提取特征词
#CountVectorizer方法
from sklearn.feature_extraction.text import CountVectorizer
corpus = ['好的学习资源（西西嘛呦）','使用scikit-learn工具计算文本TF-IDF值',
          '中文自然语言处理库','按条件筛选数据']
#将文本中的词语转换为词频矩阵
vectorizer = CountVectorizer()
#计算个词语出现的次数
X = vectorizer.fit_transform(corpus)
#获取词袋中所有文本关键词
word = vectorizer.get_feature_names()
print(word)
#查看词频结果
print(X.toarray())

#TfidfTransformer方法
from sklearn.feature_extraction.text import TfidfTransformer
#类调用
transformer = TfidfTransformer()
print(transformer)
#将词频矩阵X统计成TF-IDF值
tfidf = transformer.fit_transform(X)
#查看数据结构 tfidf[i][j]表示i类文本中的tf-idf权重
print(tfidf.toarray())
